{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the business case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Understanding the business domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ___understand the data dictionary___\n",
    "- check the work of other Data Scientists\n",
    "- talk to stakeholders\n",
    "- search\n",
    "  - [Google](https://www.google.com/)\n",
    "  - [Google Scholar](https://scholar.google.com/)\n",
    "- Company website\n",
    "- Competitor websites\n",
    "- Company intranet\n",
    "- [arxiv](https://arxiv.org/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Asking business relevant questions / Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regression?\n",
    "- Classification?\n",
    "- Both?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Defining a metric of success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data into a Python runtime using:\n",
    "\n",
    "- [Pandas](https://pandas.pydata.org/)\n",
    "- [Apache Spark](https://spark.apache.org/)\n",
    "- [Apache Arrow](https://arrow.apache.org/) for compatibility between different runtimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From different data sources:\n",
    "- CSV\n",
    "- JSON\n",
    "- [Parquet](https://parquet.apache.org/)\n",
    "- [Avro](https://avro.apache.org/)\n",
    "- AWS S3\n",
    "- HTML (via Web Scraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Loading Parquet with Apache Spark/PySpark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading `data/drug_consumption.parquet` into a `pyspark.sql.SparkSession` instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "name = 'drug_consumption'\n",
    "spark = SparkSession.builder.appName(name).getOrCreate()\n",
    "\n",
    "file = spark.read.parquet(f\"data/{name}.parquet\")\n",
    "file.createOrReplaceTempView(name)\n",
    "\n",
    "query_result = spark.sql(f\"SELECT count(id) FROM {name}\")\n",
    "query_result.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Loading CSV and Parquet with Apache Arrow Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example for a utility function which loads a CSV file into a `pandas.DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas\n",
    "import pyarrow.csv as csv\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "def read_csv(path: str) -> pandas.DataFrame:\n",
    "    \"\"\"\n",
    "    Loading a CSV file from the given path.\n",
    "    \n",
    "    :param path: The path to a CSV file.\n",
    "    :returns: A DataFrame containing the CSV data.\n",
    "    :rtype: pandas.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    table = csv.read_csv(path)\n",
    "    return table.to_pandas()\n",
    "\n",
    "\n",
    "def read_parquet(path: str) -> pandas.DataFrame:\n",
    "    \"\"\"\n",
    "    Loading a Parquet file from the given path.\n",
    "    \n",
    "    :param path: The path to a Parquet file.\n",
    "    :returns: A DataFrame containing the Parquet data.\n",
    "    :rtype: pandas.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    table = pq.read_table(path)\n",
    "    return table.pandas()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- read and understand data dictionary\n",
    "- understanding features of the dataset\n",
    "  - column names/predictors/independent variables\n",
    "  - dependent/predicted variable\n",
    "- understanding the rows/observations of the dataset\n",
    "- shape of the dataset\n",
    "  - too small\n",
    "    - get more data\n",
    "    - synthetic data generation\n",
    "    - search for additional sources\n",
    "  - too large\n",
    "    - do Principal Component Analysis (part of _Feature Engineering_) when too many features exist\n",
    "    - take random samples\n",
    "- dealing with dublicates\n",
    "  - remove dublicates\n",
    "  - for anomoly detection dublicates would be kept in they help to identify patterns over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- map data types\n",
    "  - numerical\n",
    "  - categorical\n",
    "  - same scale (time, corrency)\n",
    "- finding missing values\n",
    "- finding dublicates\n",
    "- setting dummy variables\n",
    "- type conversions\n",
    "- renaming columns\n",
    "- dropping obviously redundant or obsolete columns\n",
    "  - there might be good to reasons to not delete columns\n",
    "- handle missing data\n",
    "  - impute: i.e. take the `mean`, `meadian` on some other value\n",
    "  - delete the feature or observation\n",
    "  - enter dummy for missing values, i.e. for a categorical add extra dummy column for missing values. Say 1 Hot encoding where we put a `1` when a value is present, `0` else.\n",
    "- create _dummy_ variables for categorical values\n",
    "- ensure that for same feature categorical variables are encdeded consistently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples: Mapping Data Types with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def column_to_datetime(df: pandas.DataFrame, column_name: str) -> pandas.DataFrame:\n",
    "    \"\"\"\n",
    "    Coverting all values of a columns to \n",
    "\n",
    "    :param df: The source pandas.DataFrame.\n",
    "    :param column_name: The name of the column to transform.\n",
    "    :returns: The pandas.DataFrame passed as argument.\n",
    "    :rtype: pandas.Timestamp\n",
    "    \"\"\"\n",
    "    df[column_name] = df[column_name].apply(pd.to_datetime)\n",
    "    return df\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- summary statistics:\n",
    "  - [`pandas.DataFrame.descibe()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html)\n",
    "    - distribution of numerical features\n",
    "- using visualization for exploratory data analysis\n",
    "  - libraries/packages\n",
    "    - [matplotlib](https://matplotlib.org/)\n",
    "    - [seaborn](https://seaborn.pydata.org/)\n",
    "    - [bokeh](https://docs.bokeh.org/)\n",
    "    - [plotly](https://plot.ly/)\n",
    "- using SQL for distinct queries on the data to get a more granular look on data\n",
    "- check outliers from distributions of the plot\n",
    "- check for normality (Normal distribution)\n",
    "- advanced plots to gain more insights on the data\n",
    "- heatmap for visualizations of correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ___dimensionality reduction, Principal Comopnent Analysis (PCA)___\n",
    "- transforming/scaling skewed, continuous variables to normally distributed variables\n",
    "  - MinMaxScaler\n",
    "  - StandardScaler\n",
    "- extract from features, say from `name` feature extract `gender`\n",
    "- regrouping old categories to create new categories\n",
    "- dummy variables for categories we want to use\n",
    "- impute values for features (this can be a part of data cleaning as well as feature engineering)\n",
    "- assign weights to features denoting feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predictive Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- choose the models to use that are appropriate for the current challenge\n",
    "  - resources identified Business Understanding\n",
    "  - in Kaggle winners use ___Stacking___ to get best performance/evaluation, but in the real world this is usually not the case\n",
    "- preparation for predictive modelling\n",
    "- train/test split (90/10, 70/30, 80/20 ...)\n",
    "- balance the dataset\n",
    "- what is the model baseline performance metric?\n",
    "  - naive model should perform than a random guess\n",
    "- run the model\n",
    "  - applying PCA\n",
    "  - GridSearch\n",
    "  - KFold cross validation\n",
    "- evaluate model performance\n",
    "  - confusion metrics\n",
    "  - classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
